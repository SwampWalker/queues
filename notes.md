# Learning queue theory

I made this project to learn about queue theory, so here I collect my notes.

I'm following *Fundamentals of Queueing Theory, 5th edition*, by Shortle, Thompson, Gross and Harris.
I'm just going to refer to this as *Fundamentals*, since I don't expect to refer to anything else.

## The M/M/1 queue

Well, we should start simple. This is a poisson process so my first instinct is Poisson distribution, but
the random variable there is the events in a given interval. Instead, we want the thing that the poisson
distribution is built from: the Exponential distribution. That's even what the M means... but I needed to
read it again.

### Data structures? Event based bookkeeping.

*Fundamentals* has a good elaboration of this in section 1.6. The basic idea is to co-recursively generate
a stream of customers. Each customer has an arrival time and a service time.

That defines everything, but the interesting quantities must be derived from that.

The stream can be generated by sampling from the customer arrival distribution and adding that random
variable to the time of arrival of the previous customer. This provides `t_n`, the interarrival time
between customer `(n-1)` and customer `n`. Adding this to the arrival time of the prior customer gives
us the new arrival time `a_n`. *Fundamentals* has the first customer starting
at t=0, which is going to be equivalent due to the memoryless aspect and clearly shouldn't affect the
steady state behavior. The service time of the customer is simply a sample from the service time 
distribution, this provides `s_n`.

From here calculations are easy and iterated. The time that customer `n` starts service, `u_n`, is the time
that customer `(n-1)` leaves the system: `u_n = max(d_(n-1), a_n)`. Where we have implicitly defined the
time that a customer leaves the system as `d_n = u_n + s_n`. Then we have the items of interest: 
`wq_n = u_n - a_n`, the time in the queue and `w_n = wq_n + s_n`.

### The number of people in the system: N(t)

This can be derived from the event stream above. Actually, each "event" above defines three separate events
for customer n:

1. The time that customer n arrives `A_n`,
2. The time that customer n begins service, `U_n`,
3. The time that customer n leaves the system, `D_n`,

The ordered set of all of these `{A_n, U_n, D_n}` times defines the domain of some other time series,

1. The total number of arrivals `A(t)`,
2. The total number of departures `D(t)`,
3. The total number of customers in the system `N(t) = A(t) - D(t)`.

For deriving these time series, it seems that the time of service initiation is unimportant. The number
of people in the queue is clearly the total number of customers in the system minus one or zero, whichever
is larger.

When we sample another customer, either the arrival time is less than the time of the last customer's
departure time or it is greater or it is equal. If it is less, the arrival time is emitted and `A(t)`
is incremented. If it is equal, then `D(t)` is incremented. If it is greater, than the departure time is
emitted incrementing `D(t)` and the process starts over.

In this way, a queue of departure times is kept in memory equal to `N(t_n)`.

### The proportion of time spent with n people in the queue: p_n

This is a nice little result presented in section 3.2. It follows largely from modelling the queue
as a markov chain and deriving the state change probabilities at steady state using a sort of flow
notion. The rate of incoming customers is given by lambda, the rate of departing customers is given
by mu, this leads to a single parameter rho = lambda / mu and a handy formula for p_n:

    p_n = (1 - rho) * rho^n

This is easy to compare.

    .\target\debug\queue_counts.exe --path target/counts.txt
    python .\scripts\src\queues_analysis\proportions.py

Should probably parameterize that python script a bit... The analysis seems kind of noisy. I'm not sure
what the error bars are supposed to be.

#### I made a mistake, didn't I?

The comparison with the theoretical proportions seems like more than just noise, it seems like something
is wrong:

    n p_n      E(p_n) 
    0 0.017758 0.03333333333333344
    1 0.034762 0.03222222222222232
    2 0.033267 0.031148148148148237
    3 0.03196 0.03010987654320996
    4 0.031063 0.029106213991769627

It is hard to imagine being off by a factor of 2. Of course, we start in state 1. To get to 0,
we always have to pass through 1 and the next transition from 0 will always be to 1. If we do
not stop the simulation in state 0, then the number of times 1 was seen will always be greater
than 0. Perhaps my interpretation of p_n is wrong?

> Let p_n denote the long-term fraction of time the system is in state n.

Yeah, that sounds like the wrong interpretation. Need to sum time in state and divide by total time.
I was calculating the proportion of state transitions to i. I'm curious what that would be, but it is
not p_n.

I'm still not convinced; I lean towards having another bug... here is a sample of 10_000_000 customers:

    0 0.03420710410659557 0.03333333333333344
    1 0.033221973151713706 0.03222222222222232
    2 0.03223561829434469 0.031148148148148237
    3 0.03110375159206097 0.03010987654320996
    4 0.030187963269682684 0.029106213991769627

Shouldn't there be more precision? Hmm, let's check out the sample average lambda and mu and compare those with the
provided.

    C:\projects\star_analysis\queues\Scripts\python.exe C:/projects/queues/scripts/src/queues_analysis/sample_rates.py
    lambda, actual = 0.0016111111111111111, sampled = 0.0016127451977311228
    mu, actual = 0.0016666666666666668, sampled = 0.0016668901793021147

Looks pretty good.